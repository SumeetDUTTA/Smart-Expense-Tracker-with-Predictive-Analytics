# -*- coding: utf-8 -*-
"""expense_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PRgMJVM6UmOntmyb2WUoDcaL-uVuCzj_
"""

# !pip install catboost
# !pip install optuna
# !pip install optuna-integration[xgboost]

import numpy as np
import pandas as pd

from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor
from sklearn.model_selection import TimeSeriesSplit, train_test_split
import optuna
from optuna.integration import XGBoostPruningCallback
import xgboost as xgb
import joblib

df = pd.read_csv("Personal_Finance_Dataset.csv")

df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Quarter'] = df['Date'].dt.quarter
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['Type'] = df['Type'].astype(str).str.strip().str.lower()

df_exp = df[df['Type'] == 'expense'].copy()

# Monthly expense totals per category
monthly = (
    df_exp.groupby([df_exp['Date'].dt.to_period('M'), 'Category'])
    .agg(total_amount=('Amount', 'sum'))
    .reset_index()
)

# Convert period to timestamp
monthly['Date'] = monthly['Date'].dt.to_timestamp()

# 1. Log-scaling to stabilize variance
monthly['log_amount'] = np.log1p(monthly['total_amount'])

# 2. Lag features
for lag in [1, 2, 3, 12]:
    monthly[f'lag_{lag}'] = monthly.groupby('Category')['log_amount'].shift(lag)

# 3. Rolling averages (use shifted data to avoid leakage)
monthly['Rolling3'] = (
    monthly.groupby('Category')['log_amount']
    .transform(lambda x: x.shift(1).rolling(3, min_periods=1).mean())
)
monthly['Rolling6'] = (
    monthly.groupby('Category')['log_amount']
    .transform(lambda x: x.shift(1).rolling(6, min_periods=1).mean())
)
monthly['Rolling12'] = (
    monthly.groupby('Category')['log_amount']
    .transform(lambda x: x.shift(1).rolling(12, min_periods=1).mean())
)

# 4. Time-based cyclic features
monthly['month_num'] = monthly['Date'].dt.month
monthly['month_sin'] = np.sin(2 * np.pi * monthly['month_num'] / 12)
monthly['month_cos'] = np.cos(2 * np.pi * monthly['month_num'] / 12)

# 5. Spending trend and momentum features
monthly['trend_3'] = monthly.groupby('Category')['log_amount'].transform(lambda x: x.diff(3))
monthly['pct_change'] = monthly.groupby('Category')['log_amount'].pct_change().fillna(0)

# 6. Category ratios (proportional spending)

# Monthly total (still in log scale)
monthly['month_total'] = monthly.groupby('Date')['log_amount'].transform('sum')
# Category’s share of monthly spending
monthly['category_ratio'] = monthly['log_amount'] / monthly['month_total']

# 7. Define target = next month’s log amount
monthly['target'] = monthly.groupby('Category')['log_amount'].shift(-1)

# 8. Clean dataset - Drop rows with NaN in features or target
data = monthly.dropna(subset=[col for col in monthly.columns if col != 'Date']).reset_index(drop=True)


# 9. One-hot encode categories
data = pd.get_dummies(data, columns=['Category'], drop_first=False)

print("Feature set ready:", data.shape)
print("Columns:", data.columns[:15])

# Feature and target selection
FEATURES = [
    'lag_1', 'lag_2', 'lag_3', 'lag_12',
    'Rolling3', 'Rolling6', 'Rolling12',
    'trend_3', 'pct_change',
    'month_num', 'month_sin', 'month_cos',
    'category_ratio'
] + [col for col in data.columns if col.startswith('Category_')]

X = data[FEATURES]
y = data['target'] 

# Train/test split
test_size = 12
train_X, test_X = X[:-test_size], X[-test_size:]
train_y, test_y = y[:-test_size], y[-test_size:]


# Objective function for Optuna
def objective(trial):
    # Hyperparameters to tune
    n_estimators = trial.suggest_int('n_estimators', 200, 600)
    max_depth = trial.suggest_int('max_depth', 3, 10)
    lr = trial.suggest_float('learning_rate', 0.01, 0.3)
    reg_lambda = trial.suggest_float('reg_lambda', 0.1, 2.0)
    reg_alpha  = trial.suggest_float('reg_alpha', 0.0, 1.0)
    subsample  = trial.suggest_float('subsample', 0.7, 1.0)
    colsample  = trial.suggest_float('colsample_bytree', 0.7, 1.0)

    model = XGBRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=lr,
        reg_lambda=reg_lambda,
        reg_alpha=reg_alpha,
        subsample=subsample,
        colsample_bytree=colsample,
        random_state=42,
        eval_metric="mae",
        objective="reg:absoluteerror",
        verbosity=0,
    )

    # Weight recent data more heavily
    weights = np.linspace(0.5, 1.5, len(train_y))

    # Split last 10% for validation
    split_idx = int(len(train_X) * 0.9)
    X_train_sub, X_val_sub = train_X.iloc[:split_idx], train_X.iloc[split_idx:]
    y_train_sub, y_val_sub = train_y.iloc[:split_idx], train_y.iloc[split_idx:]
    weights_sub = weights[:split_idx]

    # Fit model normally (no early_stopping_rounds, no callbacks)
    model.fit(
        X_train_sub,
        y_train_sub,
        sample_weight=weights_sub,
        eval_set=[(X_val_sub, y_val_sub)],
        verbose=False
    )

    # Manual evaluation on validation set
    preds_val = model.predict(X_val_sub)
    mae_val = mean_absolute_error(y_val_sub, preds_val)

    return mae_val

# Run Optuna study
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=30, show_progress_bar=True)

print("\nBest trial parameters:", study.best_trial.params)
print("Best trial MAE (on validation set):", study.best_trial.value)

# Retrain final model using best parameters
best_xgb = XGBRegressor(
    **study.best_trial.params,
    random_state=42,
    eval_metric="mae",
    objective="reg:absoluteerror",
    verbosity=0,
)

weights = np.linspace(0.5, 1.5, len(train_y))

best_xgb.fit(
    train_X,
    train_y,
    sample_weight=weights,
    eval_set=[(test_X, test_y)],
    verbose=False
)

# Evaluate final model on TEST data
preds = best_xgb.predict(test_X)

# Convert log predictions back to rupees
predicted_expense_rupees = np.expm1(preds)
actual_expense_rupees = np.expm1(test_y)

print("\nPrediction sample (log scale):", preds[:5])
print("Prediction sample (rupees):", predicted_expense_rupees[:5])
print("Test sample (log scale):", test_y[:5].values)
print("Test sample (rupees):", actual_expense_rupees[:5].values)
print("Mean target value (log scale):", np.mean(test_y))
print("Mean target value (rupees):", np.mean(actual_expense_rupees))

# Calculate metrics on both scales
mae_log = mean_absolute_error(test_y, preds)
rmse_log = np.sqrt(mean_squared_error(test_y, preds))

mae_rupees = mean_absolute_error(actual_expense_rupees, predicted_expense_rupees)
rmse_rupees = np.sqrt(mean_squared_error(actual_expense_rupees, predicted_expense_rupees))

print("\nOptuna (tuned XGBoost) — evaluation on TEST set:")
print(f" MAE (log scale): {mae_log:.4f}")
print(f" RMSE (log scale): {rmse_log:.4f}")
print(f" MAE (rupees): ₹{mae_rupees:.2f}")
print(f" RMSE (rupees): ₹{rmse_rupees:.2f}")

# Save the trained model and features
model_data = {
    "model": best_xgb,
    "features": FEATURES,
    "best_params": study.best_trial.params,
    "mae_log": mae_log,
    "rmse_log": rmse_log,
    "mae_rupees": mae_rupees,
    "rmse_rupees": rmse_rupees
}

joblib.dump(model_data, "expense_forecast.pkl")
print(f"\n✅ Model saved as 'expense_forecast.pkl'")
print(f"   - Features: {len(FEATURES)}")
print(f"   - Test MAE (log): {mae_log:.4f}")
print(f"   - Test MAE (rupees): ₹{mae_rupees:.2f}")
print(f"   - Test RMSE (rupees): ₹{rmse_rupees:.2f}")